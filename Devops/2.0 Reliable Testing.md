# Steps to DevOps - Reliable Testing


## What is Reliable Testing?

> Reliable is the absence of Uncertainty. In order to break this down a little bit, Reliable is the removal of anything that would cause unknown or unaccounted for variance in the testing situation. Therefor Reliable Testing is Tests that run without Uncertainty.

We can look for sources of Uncertainty and remove them - giving us goals to meet. So, lets add some qualifications that are easier to measure:
 

**A Reliably Tested system should:**

* Be Automated.
* Be performed in an environment that:
* Cannot be interacted with manually (Immutable).
* Was just set up for this purpose and will be destroyed after (Ephemeral).
* Was generated from the same source that Production and is as close to identical to Production as possible without being Production (Ad-Hoc).


*Reliably Tested systems **DO NOT** have:*

* Manual testing phases or tests that have to be adjusted each time.
* Tests being executed in long running environments (dev, stage, test) that remain stood up between deployments.
* Tests that do not remain constant and consistent.
* Systems that require manual operations to finish setup (i.e. are not contained entirely within an Infrastructure as Code framework)


**Reliable Testing is achieved through 100% Automated testing performed in Immutable, Ephemeral, Ad-Hoc Environments.**


> #### A Quick aside about Coverage, Behavior, and 'Situations that slip through the cracks'
> 
> First off, let's be clear: **100% Reliable does not mean catches 100% of bugs** - It instead means that for the scenarios, behaviors, and inputs being tested, if all steps leading up to this goal are followed and executed properly, the tests you have will report back properly on if what they test is Green or not. And so, those tests are *100% Reliable*
> 
> As noted, Reliable Testing does not mean it catches all bugs. Reliable testing still needs *good* tests that are well written, do not impact the ability of a dev to make modifications to the code base, and cover the appropriate system behaviors, both good and bad. This does not mean the easily spoofed and mostly useless "Test Coverage" statistic most test suites can generate for you, but rather the potential actions a given app can take are properly and thoroughly explored in tests, both good and bad outcomes.
>
> Short answer is: To help Reliable Testing along, the use of Test Driven Development (Or Behavior Driven Development) will ensure that there is a good spread of tests across the behaviors a given App will encounter, which strengthens the concept of Reliable Testing.

## Automation

In order for testing to be Reliable, it **must** be 100% automated.

Does this mean that a completely automated test environment will catch 100% of the problems? Of course not. You cannot catch what you don't know you have to test for.

However, if there are tests that are manually executed, or worse - there are scenarios that are run by actual humans to test a given input, sequence of events, and the following output - these are inherently *Uncertain* because humans are inherently unreliable. If your QA team is having a bad day, overworked or under-staffed, or just one of those mornings where nothing seems to be working right, it is extremely common (dare say a given) that some tests will be slacked on.

Any amount of Uncertainty means the Testing is not Reliable. Automation removes the Uncertainty of human based testing.

### So How do we automate everything?

Let's first take note: What *kind* of tests you are running is irrelevant for this discussion. Unit tests, Integration, Regression, Contract, Chaos, Performance - it doesn't matter. The important part is that you can automate them to run *exactly the same way* every single time, and that you can control all the aspects of the tests in order to have complete Certainty in their outcome *assuming the code is correct* - which is what we are testing.

Second, when you remove manual testing from the picture, you are left with the discussion of how to test - and that is a discussion that has been hashed over 1000 times. Use whatever framework your team or enterprise wants. Use whatever methods, TDD/BDD or any other you prefer to write them, but have them written in code, so they can be run and repeated again and again and again.

I have never heard of a testing framework that cannot be run from the command line. In fact, I would hazard to say most start from a command line, and any GUI options are a tacked on afterthought. Be this `pytest` for Python or `jest` for Typscript or all the way up to `selenium-side-runner` for Selenium Front End automation, if it can be run from the command line it can be automated.


From there it is more a matter of philosophy as to what makes good tests. I prefer TDD/BDD methods, and mocks based on contracts that are established (and tested against the services they are Contracts for) but that is a discussion for a different article.


## Immutable, Ephemeral, Ad-Hoc Environment


The very first thing that is always brought up with this tenant of Reliable Testing is "Why should I waste the time and energy setting up a new env every time I want to test? Why can't we just run tests against those environments that are already there?"

The answer can be summed up in one statement: "Environment Drift introduces Uncertainty and long standing Environments **always** drift."

Any environment that stays around for more than a day will end up with drift. Developers are inherently lazy (which is why we write long winded articles like this one on how to automate things and never manually do the work again) and so will usually do the quickest thing they can think of first. When dealing with cloud environments, it's much faster to log into the Portal/Console for your cloud, and make a manual change - compared to "Figure out the IaC syntax, update the file, push it to the repo. Kick off the pipeline and wait (10, 15, 20+ minutes or more) for it to complete..." all to see if the one little change made a difference.

Of course the competent dev will go back an update the Infrastructure as Code now that they know what they want to do works, but in the meantime a change was made in the environment. If a test suite runs before the IaC is updated, the environment the tests are running in *does not match what production will be built from* - and therein lies Uncertainty - did that change affect the tests? Will it introduce a bug in Production? it's impossible to be 100% sure at test time in these scenarios, and so these tests have **Uncertainty**

> It is quite obvious that there are some environments that are extremely difficult to work with in this manner - environments that take hours to set up and build for example. These are challenges that will have to be overcome to reach a true state of Reliable Testing. It is also worth saying that this is in relation to DevOps and the concepts here may not apply completely to older, monolithic environments - Those beasts are inherently un-agile, and I don't believe that anyone truly expects to be able to apply modern philosophies to a codebase started 10, 15, 20 years ago. In these situations one simply must do their best, taking concepts from Reliable Testing and applying them as best as possible, while hoping for the start of the current buzzword 'Digital Transformation' to begin.


### What the heck do these 20 dollar words mean here?


Let's explore each word a little bit and get a better understanding.


#### Immutable

> Immutable Environments are ones that cannot be changed by any method *other* than deployment through a pipeline of some sort.

That pipeline can be as simple as `terraform apply` being run to as complex as a multi-stage, takes three plus hours to run job that builds every aspect of the application and puts it into play.

We want Immutable environments so they can't be messed with when the tests are running. We don't want the ability for someone to interact with the environment and change something in the middle of the test suite - now the tests that came before are no longer valid ( Uncertainty ) and the tests that come after are running on something different than what is going to be pushed to Production.

Long standing environments have this problem inherently - especially with big teams of developers and even worse if QA departments are also working in them. Stage, Test, Dev - all these are examples of ways that Developers and teams try to work faster by having environments up and running that can be tested against quickly - but there is never any guarantee they are the same as Production. So their tests tell us nothing about what will happen when the code reaches the the user.

And if your tests are not Reliable, then the chance of introducing bugs into Production goes way up - probably slowing down your teams overall progress more than any gain is had by re-using environments.

Immutability is best done programmatically - remove the permissions for anything other than the pipeline to make changes to this environment. Ensure that no invocation of the pipeline re-uses this environment to test

#### Ephemeral

> Ephemeral Environments are ones that are stood up to be used for a singular purpose against and then destroyed afterward.


A distinction here is that an Ephemeral environment doesn't have to be immediately destroyed after it's used. It could be left around to study if something went wrong in the tests or cleaned up in a cron job at the end of the day.


The key is that for any given invocation of a pipeline that starts a deployment, the environment was created from Infrastructure as Code specifically for it, tests were run against it, *and no other deployment will use or test against it*


We want these Environments only to exist for a short time in order to ensure they do not encounter drift. The do not persist long enough to accrue change, making tests made against them Reliable


*Note: This does not mean longstanding environments like Dev, Test, Stage cannot exist.* The only thing this means is that for the purposes of Reliable Testing, the environment is ephemeral and not left around to accrue Drift. Feel free to have these environments as part of a journey to good DevOps, but do not run your final pipeline tests in them. They can be useful for devs to explore and intentionally try to break, or to provide environments for other services to interact with in testing, but ultimately they are not needed as the journey continues.

 

#### Ad-Hoc

> An Ad-Hoc environment is one that was created for the express purpose at hand without consideration for or inclusion of any other wider factors.

Properly DRY and parameterized Infrastructure as Code can allow us to set up multiple identical environments at the same time. We can leverage this to create a Reliable testing environment - Using a base such as this is the same base that will be used to deploy Production, we can create environments whose signature and specifications match that of Production in all practical considerations, and are created for the express purpose of being tested against.

Depending on what kinds of tests and testing strategies your team is using, not every test has to be performed against a completely stood up environment. Unit Tests, even some Integration Tests probably don't have to be, for example. Where as End To End, Contract Testing, Performance, and many other types do need a fully function environment to run against. With an Ad-hoc environment - an environment set up specifically to test against - we can remove an Uncertainty of other processes running within that environment causing tests to run awry, or report Green when they really aren't. We create a Reliable testing environment.

 

### Ensuring all these aspects


How do you go about making Immutable, Ephemeral and Ad-Hoc environments anyway? Infrastructure as Code of course.

A properly DRY, Parameterized source of all resources that need to be stood up for the environment to be created. Parametrized is one of the important parts. You don't want a separate `main.tf` terraform file for Test, Dev, Prod - you want one file, that can be passed a different set of configs that change where it points - but not how it operates - so you have a single Source of Truth for your environment configuration.

If you need to do some things slightly outside that single file - like run a script to get some Secrets from a secure vault or set up a couple of different subsystems that are dependant on each other but independent modules - there is nothing wrong with that! Either test each one individually, or if that is unreasonable, then bundle them together into a single script that calls them in order *through code* and executes as need be. Infrastructure as Code does not have to be a single framework! It is possible to combine multiple things to achieve the same goal, especially with the complex technological landscape we have to navigate.

It is however vitally important that the exact same files (outside of a configuration file containing only variable values) that will be used to deploy/update Production *is the same file (with different parameters) that will be used to deploy the Ad-Hoc testing environment*

As long as you can achieve that goal, then the Immutable and Ephemeral parts can fall into line very easy. Adding the terraform file/script/whatever method to your pipeline and then having a similar destroy method to run near the end will ensure the Ephemeral-ness, and permissions being blocked/denied to adjust it from any actor other than the pipeline will take care of the Immutable.


> Need a hint on how to keep these environments from overlapping with human readable names? Prefix the names with the first 7 or so digits of the Commit Sha they come from. As your team is heading toward [Deployed By Commit](3.0%20Deployed%20By%20Commit.md) this will be a good place to go with that - As the commit to your repo kicks off the pipeline, take the sha, get the first 7 digits - unique enough for our purposes (it's the same set git shows) and prefix them onto your resource names as part of the parameters in your Infrastructure as Code. Not only will this help keep them separate in the same Account/Subscription/Global Environment but it will also allow anyone looking at failed tests to know exactly and immediately what the code looked like during this deployment by referencing that commit. A double win!

 

## Finally, the testing itself has to be good

Assumed in the original definition at the top was the fact that the test suite of a given project is actually a good test suite. You can't build a reliable testing structure if your actual tests aren't good tests. Identifying and understanding what has to be tested, how to test it, and how to do so in a repeatable, isolated, abstract manner is a discussion entirely into itself.

See the [Good Testing Strategies](1.5%20Good%20Testing%20Strategies) article for more information

### Putting it Together

So altogether we have a testing environment that cannot be modified by anyone, created by the pipeline at the it's time of need, defined by Infrastructure as Code for the express purpose of testing, that is destroyed shortly after.

How about a Checklist?


**Good Testing**

* Test Driven or Behavior Driven Development
* At the very least Unit testing of the Application, and Contract Testing of its interactions with other services (both dependency and dependant)
* The tests themselves are built so they do not have to be updated outside of internal contract/fingerprint changes
* The tests themselves do not overly interfere with a devs ability to work
* The tests are encapsulated and abstract outside of the actual inner workings of the application

 

**Automation**

* All tests are covered through code and automation
* Manual Testing and QA passes are removed in favor of code driven repeatable test suites that cannot be affected by the daily grind
* Tests are run in code based frameworks that can easily be added to a pipeline
* The Environment that the tests run against is deployed and destroyed automatically by the Pipeline

 

**Ad-Hoc Environments**

* Environments for testing should be built for the express purpose of testing and nothing else.
* They should be built from Infrastructure as Code Sources, preferably *the exact same source being used for Production* - Preferably there is only a single source!
* Prefix the environment's resources with the Commit-Sha of the commit that triggered the pipeline
* A given invocation of the pipeline sets up its own testing environment and does not use or re-use another invocations env.

 

**Ephemeral Environments**

* Environments should only exist as long as it takes the tests to run, then are destroyed
* Long standing environments such as Dev, Stage, Test are *NOT* used for Reliable Testing (though can still exist!)
* Environment stand up and destruction are handled by the pipeline itself

 

**Immutable Environments**

* No environment can be modified except by the pipeline that deploys it
* Even then, the only actions the pipeline can do is Deploy and Destroy - no modifications to existing testing environments

And finally, once these stages are reached - Continue the journey with [Deployed By Commit](3.0%20Deployed%20by%20Commit.md)


## Some Final Notes:

> Don't let perfect be the enemy of progress

You don't have to wait until you can do everything listed above before implementing it. Have your tests automated but the still run against the long-standing dev environment? Get them into the pipeline and come back to that long standing part later. Haven't yet gotten everything into Infrastructure as Code? Find the CLI commands to do the rest you would do "manually" and write a script until such time that you can move it.

This is a journey and you never complete the journey by immediately jumping to the last step. As long as your team is constantly striving to move to this goal that is more important than the speed at which you reach it.